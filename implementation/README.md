# Agente RAG Docling

Um agente CLI inteligente baseado em texto que fornece acesso conversacional a uma base de conhecimento armazenada em PostgreSQL com PGVector. Usa RAG (Retrieval Augmented Generation) para pesquisar atravÃ©s de documentos embebidos e fornecer respostas contextuais e precisas com citaÃ§Ãµes de fonte. Suporta mÃºltiplos formatos de documento incluindo arquivos de Ã¡udio com transcriÃ§Ã£o Whisper.

## ğŸ“ Novo no Docling?

**Comece com os tutoriais!** Confira a pasta [`docling_basics/`](./docling_basics/) para exemplos progressivos que ensinam os fundamentos do Docling:

1. **ConversÃ£o Simples de PDF** - Processamento bÃ¡sico de documentos
2. **Suporte a MÃºltiplos Formatos** - Manuseio de PDF, Word, PowerPoint
3. **TranscriÃ§Ã£o de Ãudio** - Speech-to-text com Whisper
4. **Chunking HÃ­brido** - Chunking inteligente para sistemas RAG

Estes tutoriais fornecem a base para entender como este agente RAG completo funciona. [**â†’ Ir para Docling Basics**](./docling_basics/)

## Recursos

- ğŸ’¬ CLI interativo baseado em texto com respostas em streaming
- ğŸ” Busca semÃ¢ntica atravÃ©s de documentos embebidos em vetor
- ğŸ“š Respostas conscientes de contexto usando pipeline RAG
- ğŸ¯ CitaÃ§Ã£o de fonte para todas as informaÃ§Ãµes fornecidas
- ğŸ”„ SaÃ­da de texto em streaming em tempo real conforme tokens chegam
- ğŸ’¾ PostgreSQL/PGVector para armazenamento escalÃ¡vel de conhecimento
- ğŸ§  HistÃ³rico de conversa mantido atravÃ©s das interaÃ§Ãµes
- ğŸ™ï¸ TranscriÃ§Ã£o de Ã¡udio com Whisper ASR (arquivos MP3)

## PrÃ©-requisitos

- Python 3.9 ou posterior
- PostgreSQL com extensÃ£o PGVector (Supabase, Neon, Postgres auto-hospedado, etc.)
- Chaves de API:
  - Chave API OpenAI (para embeddings e LLM)

## InÃ­cio RÃ¡pido

### 1. Instalar DependÃªncias

```bash
# Instalar dependÃªncias usando UV
uv sync
```

### 2. Configurar VariÃ¡veis de Ambiente

Copie `.env.example` para `.env` e preencha suas credenciais:

```bash
cp .env.example .env
```

Required variables:
- `DATABASE_URL` - PostgreSQL connection string with PGVector extension
  - Example: `postgresql://user:password@localhost:5432/dbname`
  - Supabase: `postgresql://postgres.[project-ref]:[password]@aws-0-[region].pooler.supabase.com:5432/postgres`
  - Neon: `postgresql://[user]:[password]@[endpoint].neon.tech/[dbname]`

- `OPENAI_API_KEY` - OpenAI API key for embeddings and LLM
  - Get from: https://platform.openai.com/api-keys

Optional variables:
- `LLM_CHOICE` - OpenAI model to use (default: `gpt-4o-mini`)
- `EMBEDDING_MODEL` - Embedding model (default: `text-embedding-3-small`)

### 3. Configure Database

You must set up your PostgreSQL database with the PGVector extension and create the required schema:

1. **Enable PGVector extension** in your database (most cloud providers have this pre-installed)
   ```sql
   CREATE EXTENSION IF NOT EXISTS vector;
   ```

2. **Run the schema file** to create tables and functions:
   ```bash
   # In the SQL editor in Supabase/Neon, run:
   sql/schema.sql

   # Or using psql
   psql $DATABASE_URL < sql/schema.sql
   ```

The schema file (`sql/schema.sql`) creates:
- `documents` table for storing original documents with metadata
- `chunks` table for text chunks with 1536-dimensional embeddings
- `match_chunks()` function for vector similarity search

### 4. Ingest Documents

Add your documents to the `documents/` folder. **Multiple formats supported via Docling**:

**Supported Formats:**
- ğŸ“„ **PDF** (`.pdf`)
- ğŸ“ **Word** (`.docx`, `.doc`)
- ğŸ“Š **PowerPoint** (`.pptx`, `.ppt`)
- ğŸ“ˆ **Excel** (`.xlsx`, `.xls`)
- ğŸŒ **HTML** (`.html`, `.htm`)
- ğŸ“‹ **Markdown** (`.md`, `.markdown`)
- ğŸ“ƒ **Text** (`.txt`)
- ğŸµ **Audio** (`.mp3`) - transcribed with Whisper

```bash
# Ingest all supported documents in the documents/ folder
# NOTE: By default, this CLEARS existing data before ingestion
uv run python -m ingestion.ingest --documents documents/

# Adjust chunk size (default: 1000)
uv run python -m ingestion.ingest --documents documents/ --chunk-size 800
```

**âš ï¸ Important:** The ingestion process **automatically deletes all existing documents and chunks** from the database before adding new documents. This ensures a clean state and prevents duplicate data.

The ingestion pipeline will:
1. **Auto-detect file type** and use Docling for PDFs, Office docs, HTML, and audio
2. **Transcribe audio files** using Whisper Turbo ASR with timestamps
3. **Convert to Markdown** for consistent processing
4. **Split into semantic chunks** with configurable size
5. **Generate embeddings** using OpenAI
6. **Store in PostgreSQL** with PGVector for similarity search

### 5. Run the Agent

```bash
# Run the Docling RAG Agent CLI
uv run python cli.py
```

**Features:**
- ğŸ¨ **Colored output** for better readability
- ğŸ“Š **Session statistics** (`stats` command)
- ğŸ”„ **Clear history** (`clear` command)
- ğŸ’¡ **Built-in help** (`help` command)
- âœ… **Database health check** on startup
- ğŸ” **Real-time streaming** responses

**Available commands:**
- `help` - Show help information
- `clear` - Clear conversation history
- `stats` - Show session statistics
- `exit` or `quit` - Exit the CLI

**Example interaction:**
```
============================================================
ğŸ¤– Docling RAG Knowledge Assistant
============================================================
AI-powered document search with streaming responses
Type 'exit', 'quit', or Ctrl+C to exit
Type 'help' for commands
============================================================

âœ“ Database connection successful
âœ“ Knowledge base ready: 20 documents, 156 chunks
Ready to chat! Ask me anything about the knowledge base.

You: What topics are covered in the knowledge base?
ğŸ¤– Assistant: Based on the knowledge base, the main topics include...

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
You: quit
ğŸ‘‹ Thank you for using the knowledge assistant. Goodbye!
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CLI User  â”‚â”€â”€â”€â”€â–¶â”‚  RAG Agent   â”‚â”€â”€â”€â”€â–¶â”‚ PostgreSQL  â”‚
â”‚   (Input)   â”‚     â”‚ (PydanticAI) â”‚     â”‚  PGVector   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
                    â”‚             â”‚
              â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
              â”‚  OpenAI  â”‚  â”‚  OpenAI  â”‚
              â”‚   LLM    â”‚  â”‚Embeddingsâ”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Audio Transcription Feature

Audio files are automatically transcribed using **OpenAI Whisper Turbo** model:

**How it works:**
1. When ingesting audio files (MP3 supported currently), Docling uses Whisper ASR
2. Whisper generates accurate transcriptions with timestamps
3. Transcripts are formatted as markdown with time markers
4. Audio content becomes fully searchable through the RAG system

**Benefits:**
- ğŸ™ï¸ **Speech-to-text**: Convert podcasts, interviews, lectures into searchable text
- â±ï¸ **Timestamps**: Track when specific content was mentioned
- ğŸ” **Semantic search**: Find audio content by topic or keywords
- ğŸ¤– **Fully automatic**: Drop audio files in `documents/` folder and run ingestion

**Model details:**
- Model: `openai/whisper-large-v3-turbo`
- Optimized for: Speed and accuracy balance
- Languages: Multilingual support (90+ languages)
- Output format: Markdown with timestamps like `[time: 0.0-4.0] Transcribed text here`

**Example transcript format:**
```markdown
[time: 0.0-4.0] Welcome to our podcast on AI and machine learning.
[time: 5.28-9.96] Today we'll discuss retrieval augmented generation systems.
```

## Key Components

### RAG Agent

The main agent (`rag_agent.py`) that:
- Manages database connections with connection pooling
- Handles interactive CLI with streaming responses
- Performs knowledge base searches via RAG
- Tracks conversation history for context

### search_knowledge_base Tool

Function tool registered with the agent that:
- Generates query embeddings using OpenAI
- Searches using PGVector cosine similarity
- Returns top-k most relevant chunks
- Formats results with source citations

Example tool definition:
```python
async def search_knowledge_base(
    ctx: RunContext[None],
    query: str,
    limit: int = 5
) -> str:
    """Search the knowledge base using semantic similarity."""
    # Generate embedding for query
    # Search PostgreSQL with PGVector
    # Format and return results
```

### Database Schema

- `documents`: Stores original documents with metadata
  - `id`, `title`, `source`, `content`, `metadata`, `created_at`, `updated_at`

- `chunks`: Stores text chunks with vector embeddings
  - `id`, `document_id`, `content`, `embedding` (vector(1536)), `chunk_index`, `metadata`, `token_count`

- `match_chunks()`: PostgreSQL function for vector similarity search
  - Uses cosine similarity (`1 - (embedding <=> query_embedding)`)
  - Returns chunks with similarity scores above threshold

## Performance Optimization

### Database Connection Pooling
```python
db_pool = await asyncpg.create_pool(
    DATABASE_URL,
    min_size=2,
    max_size=10,
    command_timeout=60
)
```

### Embedding Cache
The embedder includes built-in caching for frequently searched queries, reducing API calls and latency.

### Streaming Responses
Token-by-token streaming provides immediate feedback to users while the LLM generates responses:
```python
async with agent.run_stream(user_input, message_history=history) as result:
    async for text in result.stream_text(delta=False):
        print(f"\rAssistant: {text}", end="", flush=True)
```

## Docker Deployment

### Using Docker Compose

```bash
# Start all services
docker-compose up -d

# Ingest documents
docker-compose --profile ingestion up ingestion

# View logs
docker-compose logs -f rag-agent
```

## API Reference

### search_knowledge_base Tool

```python
async def search_knowledge_base(
    ctx: RunContext[None],
    query: str,
    limit: int = 5
) -> str:
    """
    Search the knowledge base using semantic similarity.

    Args:
        query: The search query to find relevant information
        limit: Maximum number of results to return (default: 5)

    Returns:
        Formatted search results with source citations
    """
```

### Database Functions

```sql
-- Vector similarity search
SELECT * FROM match_chunks(
    query_embedding::vector(1536),
    match_count INT,
    similarity_threshold FLOAT DEFAULT 0.7
)
```

Returns chunks with:
- `id`: Chunk UUID
- `content`: Text content
- `embedding`: Vector embedding
- `similarity`: Cosine similarity score (0-1)
- `document_title`: Source document title
- `document_source`: Source document path

## Project Structure

```
docling-rag-agent/
â”œâ”€â”€ cli.py                   # Enhanced CLI with colors and features (recommended)
â”œâ”€â”€ rag_agent.py             # Basic CLI agent with PydanticAI
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ ingest.py            # Document ingestion pipeline
â”‚   â”œâ”€â”€ embedder.py          # Embedding generation with caching
â”‚   â””â”€â”€ chunker.py           # Document chunking logic
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ providers.py         # OpenAI model/client configuration
â”‚   â”œâ”€â”€ db_utils.py          # Database connection pooling
â”‚   â””â”€â”€ models.py            # Pydantic models for config
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ schema.sql           # PostgreSQL schema with PGVector
â”œâ”€â”€ documents/               # Sample documents for ingestion
â”œâ”€â”€ pyproject.toml           # Project dependencies
â”œâ”€â”€ .env.example             # Environment variables template
â””â”€â”€ README.md                # This file
```